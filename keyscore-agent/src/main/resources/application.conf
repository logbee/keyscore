
keyscore {
  cluster {
    name = "keyscore"
    seed-node-host = "127.0.0.1"
    seed-node-host = ${?KEYSCORE_CLUSTER_SEED_NODE_HOST}
    seed-node-port = 2551
    seed-node-port = ${?KEYSCORE_CLUSTER_SEED_NODE_PORT}
    seed-node-port-2 = 2552
    seed-node-port-2 = ${?KEYSCORE_CLUSTER_SEED_NODE_PORT_2}
  }

  agent {
    extensions: [
      {type = "source", class = "io.logbee.keyscore.pipeline.contrib.kafka.KafkaSourceLogic"}
//      {type = "source", class = "io.logbee.keyscore.pipeline.contrib.tailin.TailinSourceLogic"} // Enable when ready for public use.
//      {type = "source", class = "io.logbee.keyscore.pipeline.contrib.ConstantSourceLogic"} // Enable when ready for public use.
      {type = "sink", class = "io.logbee.keyscore.pipeline.contrib.kafka.KafkaSinkLogic"}
      {type = "sink", class = "io.logbee.keyscore.pipeline.contrib.elasticsearch.ElasticSearchSinkLogic"}
      {type = "sink", class = "io.logbee.keyscore.pipeline.contrib.DiscardSinkLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.LoggerLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.GrokLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.AddFieldsLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.RemoveFieldsLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.RetainFieldsLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.DropRecordsLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.RetainRecordsLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.FingerprintLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.math.DifferentialQuotientLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.math.QuotientLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.decoder.CSVDecoderLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.decoder.JsonDecoderLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.encoder.JsonEncoderLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.batch.GroupByCountLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.batch.GroupByValueLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.RenameFieldFromValueLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.batch.FoldBatchLogic"}
      {type = "filter", class = "io.logbee.keyscore.pipeline.contrib.filter.textmutator.TextMutatorLogic"}
    ]
  }
}

production {
  akka {
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
    loglevel = "DEBUG"

    actor {
      provider = "cluster"
      debug {
        unhandled = on
      }
    }

    remote {
      maximum-payload-bytes = 1MB
      netty.tcp {
        hostname = "localhost"
        hostname = ${?KEYSCORE_REMOTE_HOST}
        port = 3551
        port = ${?KEYSCORE_REMOTE_PORT}

        bind-hostname = "0.0.0.0"
        bind-hostname = ${?KEYSCORE_REMOTE_BIND_HOST}
        bind-port = ""
        bind-port = ${?KEYSCORE_REMOTE_BIND_PORT}

        message-frame-size =  1MB
        send-buffer-size =  1MB
        receive-buffer-size =  1MB
        maximum-frame-size = 1MB
      }
    }

    cluster {
      roles = ["keyscore-agent"]
      seed-nodes = [
        "akka.tcp://"${keyscore.cluster.name}"@"${keyscore.cluster.seed-node-host}":"${keyscore.cluster.seed-node-port}
        "akka.tcp://"${keyscore.cluster.name}"@"${keyscore.cluster.seed-node-host}":"${keyscore.cluster.seed-node-port-2}
      ]
    }

    persistence {
      journal.plugin = "cassandra-journal"
      snapshot-store.plugin = "cassandra-snapshot-store"
    }

    extensions = [
      "akka.cluster.pubsub.DistributedPubSub"
    ]
  }


  # Properties for akka.kafka.ConsumerSettings can be
  # defined in this section or a configuration section with
  # the same layout.
  akka.kafka.consumer {
    # Tuning property of scheduled polls.
    poll-interval = 50ms

    # Tuning property of the `KafkaConsumer.poll` parameter.
    # Note that non-zero value means that blocking of the thread that
    # is executing the stage will be blocked.
    poll-timeout = 50ms

    # The stage will be await outstanding offset commit requests before
    # shutting down, but if that takes longer than this timeout it will
    # stop forcefully.
    stop-timeout = 30s

    # How long to wait for `KafkaConsumer.close`
    close-timeout = 20s

    # If offset commit requests are not completed within this timeout
    # the returned Future is completed `TimeoutException`.
    commit-timeout = 15s

    # If the KafkaConsumer can't connect to the broker the poll will be
    # aborted after this timeout. The KafkaConsumerActor will throw
    # org.apache.kafka.common.errors.WakeupException, which can be handled
    # with Actor supervision strategy.
    wakeup-timeout = 10s

    # Fully qualified configurations path which holds the dispatcher configuration
    # to be used by the KafkaConsumerActor. Some blocking may occur.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
    # can be defined in this configuration section.
    kafka-clients {
      # Disable auto-commit by default
      enable.auto.commit = true
      auto.commit.interval.ms = 10000
    }
  }

  # Properties for akka.kafka.ProducerSettings can be
  # defined in this section or a configuration section with
  # the same layout.
  akka.kafka.producer {
    # Tuning parameter of how many sends that can run in parallel.
    parallelism = 100

    # How long to wait for `KafkaProducer.close`
    close-timeout = 60s

    # Fully qualified configurations path which holds the dispatcher configuration
    # to be used by the producer stages. Some blocking may occur.
    # When this value is empty, the dispatcher configured for the stream
    # will be used.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
    # can be defined in this configuration section.
    kafka-clients {
    }
  }
}