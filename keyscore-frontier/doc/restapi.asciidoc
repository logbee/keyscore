= Description

With the REST API streams can be created and filters can be modified.

== Tools
For REST API testing, a browser extension is useful. We recommend to use the following extensions: +

* Chrome:   Restlet Client by Restlet
* Firefox:  RESTED by Espen H


== Create a stream

=== URL

The URL should have the following structure: http://<HOST>:<PORT>/stream/<UUID> +
A valid URL to create a new stream would look like: +
http://localhost:4711/stream/fff34f64-1550-449f-8c13-4058ae6b185a

=== Method
A stream can be created with the PUT method.

=== Body
A stream contains


* stream id
* stream name
* stream description
* source configuration
* sink configuration
* list of filter configurations


==== Stream ID
Every stream has a unique UUID. The UUID should match the UUID in the URL.
[source,json]
 "id":"fff34f64-1550-449f-8c13-4058ae6b185a",
==== Stream Name and Description
Simple Strings to describe the Stream.

==== Source,Sink and Filter Configuration
Every source,sink or filter has the following fields:

* id : UUID
* kind : describes the filter type (e.g "AddFieldsFilter")
* parameters: List of parameters to configure the filter.

===== Parameters
Configuration parameters consist of three fields:

* name of the parameter as string
* value of the parameter
* a field called parameterType representing the values type

Possible values for parameterType are:

* string,boolean,int,float,map[string,string],list[string]

Examples of different source,sink or filter configurations follow below.

==== Source
A source can be built from different types.

===== Kafka Source
We recommend using a kafka source, because with kafka the probability of loosing data is very small. +
The offset will be increased only after all data is processed in the pipeline. +
So if something went wrong during the data processing in the pipeline,
the data that failed is still in kafka and can be processed again.
[source,json]
 "source":{
     "id":"643e0b22-9f18-4c6c-ac3c-9fc79df9471d",
     "kind":"KafkaSource",
     "parameters":[
       {
         "name":"sourceTopic",
         "value":"testfluentd",
         "parameterType":"string"
       },
       {
         "name":"groupID",
         "value":"keyscore-consumer_newconfig",
         "parameterType":"string"
       },
       {
         "name":"offsetCommit",
         "value":"earliest",
         "parameterType":"string"
       },
       {
         "name":"bootstrapServer",
         "value":"testserver:9092",
         "parameterType":"string"
       }
     ]

   }

==== Sink
All the processed data will be pushed in the specified sink.

===== Kafka Sink

[source,json]
 "sink":{
     "id":"48ab7a30-7387-4fee-b821-96dfecd411d7",
     "kind":"KafkaSink",
     "parameters":[
       {
         "name":"sinkTopic",
         "value":"testfluentd",
         "parameterType":"string"
       },
       {
         "name":"bootstrapServer",
         "value":"testserver:9092",
         "parameterType":"string"
       }
     ]
   }

==== Body
The body contains all the filter you want to apply on the streaming data. +
It can be only one filter or as many as you want.
[source,json]
"filter":[
    {},
    {}
 ],

===== Retain_Fields Filter
After the retain filter is processed, only the fields in the "fields_to_remain" list will be present in the streaming data. +
All other fields are not longer in the streaming data.
[source,json]
 "id":"26d4d561-4cfc-42d6-9cf4-521e66586317",
 "kind":"RetainFieldsFilter",
 "parameters":[
     {
       "name":"fieldsToRetain",
       "value":[
         "fieldX",
         "timeField"
       ],
       "parameterType":"list[string]"
     }
 ]

===== Add_Fields Filter
This filter adds new fields with the given value to the streaming data. +
All older fields are retained.
[source,json]
 "id":"26d4d561-4cfc-42d6-9cf4-521e66586317",
 "kind":"AddFieldsFilter",
 "parameters":[
     {
       "name":"fieldToAdd",
       "value":{
         "fieldX":"valueX",
         "fieldY":"valueY"
       },
       "parameterType":"map[string,string]"
     }
 ]

===== Remove_Fields Filter
This filter removes the specified fields from the streaming data. +
All other fields are retained.
[source,json]
"id":"26d4d561-4cfc-42d6-9cf4-521e66586317",
"kind":"RemoveFieldsFilter",
"parameters":[
    {
      "name":"fieldsToRemove",
      "value":[
        "fieldX",
        "timeField"
      ],
      "parameterType":"list[string]"
    }
]


===== Grok_Fields Filter
The grok filter applies the specified regex pattern on the specified fields and extracts the results in a new field. +
The other fields are retained. +
This filter is pausable.

[source,json]
"id":"26d4d561-4cfc-42d6-9cf4-521e66586317",
"kind":"GrokFilter",
"parameters":[
    {
      "name":"isPaused",
      "value":false,
      "parameterType":"boolean"
    },
    {
      "name":"pattern",
      "value":"(?<newmessage>.*)",
      "parameterType":"string"
    },
    {
      "name":"fieldNames",
      "value":["message"],
      "parameterType":"list[string]"
    }
]


== Change a filter
=== URL
The url must contain the keyword filter: http://<HOST>:<PORT>/filter/<UUID_OF_FILTER>

=== Method
The standard method to edit a filter is PUT.

=== Body
To change a filter, edit the specific model parameters for the filter. +
These are all parameters that were used to create a filter except of the filter_type and the filter_id. +

Here is an example body how to edit a grok_field filter:
[source,json]
{
    "isPaused":"false"
    "fieldNames":[
        "fieldWithIP",
        "fieldX"
    ],
    "pattern":".*(?<ipsuffix>\\d{0,2})"
}
